{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# # options.add_argument('-–headless')\n",
    "# # options.add_argument('-–no-sandbox')\n",
    "# options.add_argument('-–disable-dev-shm-usage')\n",
    "# dr = webdriver.Chrome(r\"chrome.exe\",options=options)\n",
    "# # dr.maximize_window()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # options.binary_location = 'chrome.exe'\n",
    "# # driver = webdriver.Chrome(executable_path='chrome.exe',chrome_options=options) \n",
    "# # driver.get('http://www.google.com/xhtml');\n",
    "# # driver.quit();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      https://www.sciencedirect.com/journal/computer...\n",
       "1      https://www.sciencedirect.com/journal/computer...\n",
       "2      https://www.sciencedirect.com/journal/computer...\n",
       "3      https://www.sciencedirect.com/journal/computer...\n",
       "4      https://www.sciencedirect.com/journal/computer...\n",
       "                             ...                        \n",
       "563    https://www.sciencedirect.com/journal/computer...\n",
       "564    https://www.sciencedirect.com/journal/computer...\n",
       "565    https://www.sciencedirect.com/journal/computer...\n",
       "566    https://www.sciencedirect.com/journal/computer...\n",
       "567    https://www.sciencedirect.com/journal/computer...\n",
       "Length: 568, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取索引\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "\n",
    "html_1 = etree.parse('./backup/11/CC_Issues.html',etree.HTMLParser())\n",
    "html_2 = etree.parse('./backup/11/CC_Issues-part2.html',etree.HTMLParser())\n",
    "html_3 = etree.parse('./backup/11/CC_Issues-part3.html',etree.HTMLParser())\n",
    "data_1 = html_1.xpath('//*[@id=\"all-issues\"]/ol//a//@href')\n",
    "data_2 = html_2.xpath('//*[@id=\"all-issues\"]/ol//a//@href')\n",
    "data_3 = html_3.xpath('//*[@id=\"all-issues\"]/ol//a//@href')\n",
    "data = pd.Series(data_1+data_2+data_3)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lxml import etree\n",
    "\n",
    "# text = '''\n",
    "# <div>\n",
    "#     <ul>\n",
    "#         <li class=\"sp item-0\" name=\"one\"><a href=\"www.baidu.com\">baidu</a>\n",
    "# '''\n",
    "\n",
    "# html = etree.HTML(text)\n",
    "# result = html.xpath('//li[contains(@class,\"item-0\") and @name=\"one\"]/a/text()')#使用and操作符将两个条件相连。\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "\n",
    "\n",
    "# def find_year(da):\n",
    "#     return da.index.str.startswith(\"2020\")==1\n",
    "\n",
    "#存储为excel\n",
    "writer = pd.ExcelWriter(r'D:/jupyter_save/data.xlsx')\n",
    "# da['date'] = da['year'].map(str) + \"-\" + da['month']\n",
    "# da.set_index('date', inplace = True)\n",
    "#str.replace('*','*')\n",
    "data = data.astype('str')\n",
    "print(data.dtype)\n",
    "data.to_excel(writer,sheet_name = 'median_save',index=False)\n",
    "writer.save()\n",
    "#test\n",
    "# da.loc[find_year,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#数据爬取处理预备\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "\n",
    "\n",
    "path = pd.read_excel('./data.xlsx')\n",
    "path = path[0]\n",
    "length=len(path)\n",
    "print(length)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "# res = requests.get('https://www.sciencedirect.com/journal/computer-communications/issues',headers=gheaders)\n",
    "# html = etree.HTML(res.text)\n",
    "# result = etree.tostring(html)\n",
    "# print(result.decode('UTF-8'))\n",
    "# data = html.xpath('//*')\n",
    "# data\n",
    "#html.xpath('//tr//@href')\n",
    "def get_paperinfo(path):\n",
    "#    for x in range(0,length):\n",
    "    paper_data = []\n",
    "    for x in range(0,568):\n",
    "        time.sleep(1)\n",
    "        print(x)\n",
    "        gheaders = {\n",
    "            'Referer': 'https://www.sciencedirect.com/journal/computer-communications/issues?',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'\n",
    "            }\n",
    "        issue_url = path.loc[x]\n",
    "        print(issue_url)\n",
    "        issue_res = requests.get(issue_url,headers=gheaders)\n",
    "        issue_html = etree.HTML(issue_res.text)\n",
    "        paper_data = paper_data + issue_html.xpath('//*[@id=\"article-list\"]/form/div/div[2]/ol//a//@id')\n",
    "    save = pd.Series(paper_data)\n",
    "    paperid = pd.ExcelWriter(r'D:/jupyter_save/paperid.xlsx')\n",
    "    save.to_excel(paperid,sheet_name = 'median_file',index=False)\n",
    "    paperid.save()\n",
    "\n",
    "get_paperinfo(path)\n",
    "\n",
    "print('finish')\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       S0140366420318831\n",
       "1       S0140366420318879\n",
       "2       S0140366420318880\n",
       "3       S0140366420318491\n",
       "4       S0140366420318909\n",
       "              ...        \n",
       "8905     0140366478901299\n",
       "8906     0140366478901305\n",
       "8907     0140366478901317\n",
       "8908     0140366478901329\n",
       "8909     014036647890186X\n",
       "Name: 0, Length: 8910, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_handle = pd.read_excel(r'./paperid.xlsx')\n",
    "id_data = id_handle[0]\n",
    "id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "import time\n",
    "id_handle = pd.read_excel(r'./paperid.xlsx')\n",
    "paper_id = id_handle[0]\n",
    "info_creat = pd.DataFrame({\n",
    "                          'date':[],\n",
    "                          'Name':[],\n",
    "                          'Keyword_0':[],\n",
    "                          'Keyword_1':[],\n",
    "                          'Keyword_2':[],\n",
    "                          'Keyword_3':[],\n",
    "                          'Keyword_4':[]                                                       \n",
    "                         })\n",
    "info_save = info_creat\n",
    "def get_detail(paper_id,info_save,info_creat):\n",
    "#    for x in range(0,length):\n",
    "    median_info = info_creat\n",
    "    print('期刊数如下')\n",
    "    for x in range(6000,8910):\n",
    "        time.sleep(1)\n",
    "        print(x)\n",
    "        gheaders = {\n",
    "            'Referer': 'https://www.sciencedirect.com/journal/computer-communications/issues?',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'\n",
    "            }\n",
    "        paper_url = 'https://www.sciencedirect.com/science/article/abs/pii/'+paper_id[x]\n",
    "        print(paper_url)\n",
    "        paper_res = requests.get(paper_url,headers=gheaders)\n",
    "        paper_html = etree.HTML(paper_res.text)\n",
    "        paper_data = paper_html.xpath('//*[@class=\"keyword\"]//span/text()')\n",
    "        if paper_data != []:\n",
    "            paper_name = paper_html.xpath('//*[@id=\"screen-reader-main-title\"]/span/text()')\n",
    "            paper_date = paper_html.xpath('//*[@id=\"publication\"]/div[2]/div/text()')\n",
    "            paper_name=''.join(paper_name)\n",
    "            paper_date=''.join(paper_date).split(',')\n",
    "            paper_date = paper_date[1]\n",
    "            print(paper_name)\n",
    "            table_creat = pd.DataFrame({'date':[paper_date],\n",
    "                                        'Name':[paper_name],                                                   \n",
    "                                        })\n",
    "            if len(paper_data)<15:\n",
    "                for i in range(0,len(paper_data)):\n",
    "                    name_i = str(i)\n",
    "                    table_creat.loc[:,\"Keyword_\"+name_i]=[paper_data[i]]\n",
    "                median_info = pd.concat([median_info,table_creat],ignore_index = True)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        info_save = pd.concat([info_save,median_info],ignore_index = True)\n",
    "        median_info = info_creat\n",
    "    return info_save \n",
    "info_data = get_detail(paper_id, info_save,info_creat)\n",
    "info_data.set_index('date', inplace = True)\n",
    "paper_info = pd.ExcelWriter(r'D:/jupyter_save/paper_info.xlsx')\n",
    "info_data.to_excel(paper_info,sheet_name = 'median_file')\n",
    "paper_info.save()\n",
    "runned_flag = True\n",
    "print('Finish')\n",
    "#         issue_html = etree.HTML(issue_res.text)\n",
    "#         paper_data = issue_html.xpath('//*[@id=\"article-list\"]/form/div/div[2]/ol//a//@id')\n",
    "#         print(paper_data)\n",
    "#     return paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "#建立此期刊对应sheet且会初始化！！！\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "\n",
    "This = pd.DataFrame({\n",
    "                          'date':[],\n",
    "                          'Name':[],\n",
    "                          'Keyword_0':[],\n",
    "                          'Keyword_1':[],\n",
    "                          'Keyword_2':[],\n",
    "                          'Keyword_3':[],\n",
    "                          'Keyword_4':[]                                                       \n",
    "                         })\n",
    "writer = pd.ExcelWriter(r'D:/jupyter_save/total_info.xlsx')\n",
    "This.to_excel(writer,sheet_name = 'save',index=False)\n",
    "writer.save()\n",
    "print('Finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "runned_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "#分批录入\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "if runned_flag ==True:\n",
    "    merged_writer = pd.ExcelWriter(r'D:/jupyter_save/total_info.xlsx')\n",
    "    merge_info_now = pd.read_excel(r'./paper_info.xlsx')\n",
    "    merge_info_pro = pd.read_excel(r'./total_info.xlsx')\n",
    "    merge_allinfo = pd.concat([merge_info_pro,merge_info_now],ignore_index = True)\n",
    "    merge_allinfo.to_excel(merged_writer,sheet_name = 'save',index=False)\n",
    "    merged_writer.save()\n",
    "    runned_flag =False\n",
    "    print('Finish')\n",
    "else:\n",
    "    print('无需重复录入')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2020October\n",
       "1        2020October\n",
       "2        2020October\n",
       "3        2020October\n",
       "4        2020October\n",
       "            ...     \n",
       "5760    1982February\n",
       "5761    1982February\n",
       "5762    1982February\n",
       "5763    1982February\n",
       "5764    1982February\n",
       "Name: date, Length: 5765, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "\n",
    "test1 = pd.ExcelWriter(r'D:/jupyter_save/total_info.xlsx')\n",
    "test2 = pd.read_excel(r'./total_info.xlsx')\n",
    "info = test2['date']\n",
    "\n",
    "info_fix = info.map(str).str.split(' ')\n",
    "for i in range(0,len(info_fix)):\n",
    "    median = info_fix.loc[i]\n",
    "    median = median[len(median)-1] + median[len(median)-2]\n",
    "    info_fix.loc[i] = median\n",
    "info_fix\n",
    "# info_fix = info['Date'].str.replace(\"[0123456789.]\",\"\").str.replace('nan','')\n",
    "# info_fix = info_fix.str.replace('Jan','01').str.replace('Feb','02').str.replace('Mar','03').str.replace('Apr','04').str.replace('May','05').str.replace('Nov','11')\n",
    "# info_fix = info_fix.str.replace('Jun','06').str.replace('Jul','07').str.replace('Aug','08').str.replace('Sep','09').str.replace('Oct','10').str.replace('Dec','12')\n",
    "# info_fix = info_fix.str.replace(\"[abcehilmorstuy]\",\"\")\n",
    "# info['Date'] = info_fix\n",
    "# info['Date'] = info['Year'].map(str)  +'-'+ info['Date'].map(str)\n",
    "# info['Date'] = info['Date'].str.replace('\\n','')\n",
    "# del info['Month']\n",
    "# del info['Day']\n",
    "# del info['Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2020-10\n",
      "1       2020-10\n",
      "2       2020-10\n",
      "3       2020-10\n",
      "4       2020-10\n",
      "         ...   \n",
      "5760    1982-02\n",
      "5761    1982-02\n",
      "5762    1982-02\n",
      "5763    1982-02\n",
      "5764    1982-02\n",
      "Name: Date, Length: 5765, dtype: object\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#统一不同时期的格式并存储\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwt\n",
    "from lxml import etree\n",
    "import json\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "def add_sheet(data, excel_writer, sheet_name):\n",
    "    book = openpyxl.load_workbook(excel_writer.path)\n",
    "    excel_writer.book = book\n",
    "    data.to_excel(excel_writer=excel_writer, sheet_name=sheet_name, index=True, header=True)\n",
    "    excel_writer.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "writer = pd.ExcelWriter(r'D:/jupyter_save/Final_info.xlsx', engine='openpyxl')\n",
    "info = pd.read_excel(r'./total_info.xlsx')\n",
    "median = info['date']\n",
    "info_fix = median.map(str).str.split(' ')\n",
    "for i in range(0,len(info_fix)):\n",
    "    median = info_fix.loc[i]\n",
    "    median = median[len(median)-1] + median[len(median)-2]\n",
    "    info_fix.loc[i] = median\n",
    "\n",
    "\n",
    "info_fix = info_fix.str.replace('Jan','-01').str.replace('Feb','-02').str.replace('Mar','-03').str.replace('Apr','-04').str.replace('May','-05').str.replace('Nov','-11')\n",
    "info_fix = info_fix.str.replace('Jun','-06').str.replace('Jul','-07').str.replace('Aug','-08').str.replace('Sep','-09').str.replace('Oct','-10').str.replace('Dec','-12')\n",
    "info_fix = info_fix.str.replace(\"[abcehilmorstuy]\",\"\")\n",
    "info['Date'] = info_fix\n",
    "info['Date'] = info['Date'].map(str)\n",
    "# info['Date'] = info['Date'].str.replace('\\n','')\n",
    "del info['date']\n",
    "print(info['Date'])\n",
    "info.set_index('Date',inplace = True)\n",
    "\n",
    "add_sheet(info, writer, 'Computer Communications')\n",
    "# info.to_excel(writer,sheet_name = 'IEEE Transactions on Networking',index=True)\n",
    "# writer.save()\n",
    "print('finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
